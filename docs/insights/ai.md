# AI

## 机器学习

机器学习的三个要素：模型、训练数据、学习方式。

模型简单理解就是一个数学函数，这个函数的参数非常多，多达几千万、几亿个。

训练模型就是利用大量的训练数据来调整这个数学函数的参数。通过训练集的调教，让这几亿个参数达到较为理想的状态，使得在测试集中输入指定的数据就能得到预期的输出。

“训练”一词是以人为主体的表示，如果以“机器”为主体，则称之为“学习”。

机器学习的方式主要分为三种：分别是有监督学习 (Supervised Learning)、无监督学习 (Unsupervised Learning) 和强化学习 (Reinforcement Learning)。

- 有监督学习：指的是我们在给机器提供训练数据时，提供的数据是带有标签或类别信息的，这样机器才能够学习输入和输出之间的映射关系，从而对新的输入进行预测。有监督学习的应用场景非常广泛，如图像识别、股价预测等。
- 无监督学习：即提供给机器训练的数据是未被标记的数据，由机器自己通过某些算法发现数据之间的潜在结构与规律。无监督学习的应用场景有异常检测、推荐系统等。
- 强化学习：是一种让机器自己学习如何做出正确决策的方式。就像人类玩《飞机大战》这个游戏一样，不同的操作会得到环境的不同反馈（扣分或加分），基于环境反馈，机器会不断调整、优化自己的操作，最终达到获取最高分的目的。强化学习适用于那些目标不确定、环境动态变化、需要长期规划的问题，如自动驾驶、机器人控制、游戏 AI 等。

实际应用中模型是由非常多的数学函数以网状或更加复杂的形状形成的一个拓扑结构，由于这种拓扑结构与人的神经网络结构非常相似，我们称之为人工神经网络（Artificial Neural Network，ANN）。人工神经网络根据实际应用场景的不同发展出多种形态，应用最广泛的神经网络有：前馈神经网络（Feedforward Neural Network, FNN）、卷积神经网络（Convolutional Neural Network，CNN）和循环神经网络（Recurrent Neural Network，RNN）。

- 前馈神经网络：前馈神经网络也被称为多层感知器（Multilayer Perceptron, MLP），是一种最简单且最原汁原味的经典神经网络，它由多个神经元层组成，每一层神经元接收上一层的输出作为输入，通过权重和激活函数计算得出输出，传递给下一层神经元。前馈神经网络通常由输入层、若干个隐藏层和输出层组成。每个神经元的计算公式可以表达为 `Y=f(a1*x1+a2*x2+a3*x3+……+an*xn+b)`，其中 x 为输入，a 为权重，b 为偏置，f 为激活函数(如 ReLU 函数、sigmoid 函数等)。在训练过程中，神经网络通过学习调整权重和偏置，使得预测结果与真实值之间的误差最小化。前馈神经网络最经典例子就是数字识别，即我们随便给出一张上面写有数字的图片并作为输入，由它最终给出图片上的数字到底是几。
- 卷积神经网络常用于处理具有网格状结构的数据，如图像、音频、文本等。其基本组成部分包括卷积层、池化层和全连接层。其中，卷积层通过卷积操作对输入数据进行特征提取，池化层则对卷积层的输出进行降采样，减少特征数量和计算量，全连接层（就是上文所说的前馈神经网络）则将特征映射到输出类别上。卷积神经网络由于其强大的特征提取能力被广泛的应用于图像识别上（前馈神经网络能够处理体积小、简单的图像，但对于体积大、复杂的图像还是得由卷积神经网络来处理）。
- 循环神经网络是一类具有循环连接的神经网络，能够处理序列数据，如语音、文本和时间序列数据等。与前馈神经网络不同，它能够将网络前一序列的输入保留下来，作为本次序列输入的一部分，使其具有记忆能力，从而实现对序列数据的建模和预测。循环神经网络由于其具有记忆的特点被广泛应用于自然语言处理领域。

## GPT

GPT 即 Generative Pre-trained Transformer，是由 OpenAI 团队开发的一种基于自然语言处理技术的生成式、预训练、大语言模型。

判别式模型关注如何将输入数据映射到标签或类别，例如分类、回归等问题；而生成式模型则关注如何学习数据的分布，以便生成新的数据。ChatGPT 能够从大量的语料库中学习到复杂的语言结构和上下文信息，从而生成新的符合语言规则和语境的文本。

预训练模型是指该模型预先从大规模的语料库中通过无监督的方式学习语言的内在规律和模式，其能够学习到数据集的统计规律并提取出数据中的特征。这样，在进行具体任务的时候，GPT 可以直接使用已经学习到的知识，从而提高效率和准确性。

大模型指的是具有非常庞大的参数量和计算量的机器学习模型。这些模型通常需要在大规模的数据集上进行训练，以便能够学习到数据中的复杂模式和规律，并在各种任务中取得优秀的性能表现。ChatGPT 由于其拥有 1750 亿个参数和超过 45TB 的训练数据所以被称之为大模型。

语言模型区别与图像模型、语音模型等，是一种用来预测自然语言文本序列的概率分布的计算模型。简单来说，它就是一个能够根据前面的文本内容预测下一个可能出现的词或字的模型。语言模型通常被用于自然语言处理任务，比如语音识别、机器翻译、文本生成、文本分类等。

Transformer 是一种用于自然语言处理的神经网络模型，由 Google 提出，目的主要是为了解决循环神经网络在处理长序列时存在的一些问题（简单来说，循环神经网络无法记住一段文本中较早的单词并与当前的单词进行关联）。Transformer 模型的核心是自注意力机制（self-attention mechanism），它可以帮助计算机更好地理解数据中不同元素之间的关系。举个例子，当计算机阅读一段文字时，自注意力机制可以找出哪些单词与其他单词之间的关系更密切，解决了单词间的长距离依赖问题，从而更好地理解这段文字。

GPT 的基本原理：自回归生成，即先用模型预测下一个词是什么，然后把预测出来的词代入模型，去预测下一个词是什么，不断迭代。像是一个递归版的“单字接龙”。ChatGPT 喜欢絮絮叨叨一大堆，或者重复我们所说的话，这不光是为了告诉你解题思路，而更是为了将这段信息作为上文的补充，再从中提取关键信息，以便进一步生成正确的结果。

在训练过程中，学习材料并不会被保存在模型中，学习材料的作用只是「调整模型」以得到「通用模型」。想要让 GPT 能够应对无数未见情况，就必须提供数量足够多、种类足够丰富，质量足够高的学习材料，否则它将无法学到通用语言规律。

ChatGPT 训练过程主要分成三步：

- 无监督学习阶段：互联网爬取网页(约 31 亿个)、书籍、维基百科、博客、新闻、Github 代码等，数据来源庞大且公开，但可能含有有害内容、部份内容质量不高。
- 监督学习阶段：用人工专门写好的「优质对话范例」让它再次学习，这些范例需要人工专门编写，价格昂贵，数量有限，所能提供的语言多样性不足，可能难以让模型学到广泛适用的语言规律，也无法涉猎各个领域。
- 基于人类反馈的强化学习：人工给回答打分，并且对结果进行排序，利用这些「人类排序结果」重新调整模型。

ChatGPT 超能力的“涌现”：

- 超大语言模型意外掌握了“理解指令要求”、“理解例子要求”的能力。这种现象被称为“语境内学习（In-context Learning）”。
- 当 ChatGPT 无法回答一个大问题时，若要求它分步思考，它就可以一步步连续推理，且最终答对的可能性大幅提升，该能力也叫“思维链”。类似于人类的分治思想。
- ChatGPT 让 AI 第一次看似拥有了“乌鸦”模式（观察、感知、认知、学习、推理、执行）的智能，而不仅仅只是像鹦鹉那样学舌，它看上去像是真的会思考了。

在单字接龙的小模型中，并没有觉醒出“理解”和“推理”的能力，但在超大模型中，却突然展现。因此专家用“涌现”这个词来描述这些能力的出现。

人脑中单个神经元的工作方式特别简单，就是向下一个神经元放电，但是通过大力出奇迹的方式堆砌几百亿的神经元以后，意识就出现了。

## Prompt

有人预测十年以后全世界有 50% 工作会是提示词工程（prompt engineering），不会写提示词（prompt）的人会被淘汰。如何通过自然语言把问题向机器描述清楚，将成为以后工作中的基本技能。
